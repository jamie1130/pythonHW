{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Markov Models of Natural Language\n",
    "\n",
    "### Name: [Jiaxin Luo]\n",
    "### Collaborators: [Your collaborators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework focuses on topics related to string manipulation, dictionaries, and simulations. \n",
    "\n",
    "I encourage collaborating with your peers, but the final text, code, and comments in this homework assignment should still be written by you.\n",
    "\n",
    "Submission instructions: \n",
    "- Submit `HW2.py` on Gradescope under \"HW2 - Autograder\". Do **NOT** change the file name. The grade you see is the grade you get for the accuracy portion of your code (so no surprises). The style and readability of your code will be checked by the reader aka human grader.\n",
    "- Convert this notebook into a pdf file and submit it on GradeScope under \"HW2 - PDF\". Make sure your text outputs in the latter problems are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Many of you may have encountered the output of machine learning models which, when \"seeded\" with a small amount of text, produce a larger corpus of text which is expected to be similar or relevant to the seed text. For example, there's been a lot of buzz about the new [GPT-3 model](https://en.wikipedia.org/wiki/GPT-3), related to its [carbon footprint](https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/#2781c1b16b43), [bigoted tendencies](https://medium.com/fair-bytes/how-biased-is-gpt-3-5b2b91f1177), and, yes, impressive (and often [humorous](https://aiweirdness.com/)) [ability to replicate human-like text in response to prompts.](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/) \n",
    "\n",
    "We are not going to program a complicated deep learning model, but we will construct a much simpler language model that performs a similar task. Using tools like iteration and dictionaries, we will create a family of **Markov language models** for generating text. For the purposes of this assignment, an $n$-th order Markov model is a function that constructs a string of text one letter at a time, using only knowledge of the most recent $n$ letters. You can think of it as a writer with a \"memory\" of $n$ letters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports your functions defined in HW2.py \n",
    "\n",
    "from HW2 import count_characters, count_ngrams, markov_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our training text for this exercise comes from Jane Austen's novel *Emma*, which Professor Chodrow retrieved from the archives at ([Project Gutenberg](https://www.gutenberg.org/files/158/158-h/158-h.htm#link2H_4_0001)). Intuitively, we are going to write a program that \"writes like Jane Austen,\" albeit in a very limited sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-full.txt', 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Define `count_characters` in HW2.py\n",
    "\n",
    "Write a function called `count_characters` that counts the number of times each character appears in a user-supplied string `s`. Your function should loop over each element of the string, and sequentually update a `dict` whose keys are characters and whose values are the number of occurrences seen so far.  Your function should then return this dictionary. \n",
    "\n",
    "You may know of other ways to achieve the same result. However, you should use the loop approach, since this will generalize to the next exercise.\n",
    "\n",
    "*Note: while the construct `for character in s:` will work for this exercise, it will not generalize to the next one. Use `for i in range(len(s)):` instead.* \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_characters(\"Torto ise!\")\n",
    "{'T': 1, 't' : 1, 'o' : 2, 'r' : 1, 'i' : 1, 's' : 1, 'e' : 1, ' ': 1, '!': 1}\n",
    "```\n",
    "\n",
    "***Hint***: Yes, you did a problem very similar to this one on HW1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 1, 'o': 2, 'r': 1, 't': 1, ' ': 1, 'i': 1, 's': 1, 'e': 1, '!': 1}\n"
     ]
    }
   ],
   "source": [
    "# test your count_characters\n",
    "print(count_characters(\"Torto ise!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does 't' appear in Emma? How about '!'?\n",
    "\n",
    "How many different types of characters are in this dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 647, 'H': 1740, 'A': 708, 'P': 257, 'T': 1132, 'E': 1501, 'R': 215, ' ': 155119, 'I': 4013, '\\n': 4711, 'm': 17907, 'a': 53664, 'W': 1355, 'o': 52894, 'd': 28329, 'h': 40827, 'u': 20606, 's': 41556, 'e': 84502, ',': 12020, 'n': 46986, 'c': 14815, 'l': 27537, 'v': 7645, 'r': 40699, 'i': 42586, 'w': 14933, 't': 58067, 'f': 14598, 'b': 10532, 'p': 10284, 'y': 15268, 'g': 13525, 'x': 1346, ';': 2353, '-': 574, '.': 8881, 'S': 951, 'q': 895, '’': 1114, 'M': 2795, 'B': 598, 'j': 688, 'k': 4351, '—': 3102, ':': 174, '?': 621, '(': 107, ')': 107, 'L': 133, 'O': 304, 'N': 300, '“': 2099, '!': 1063, '”': 2090, 'J': 432, 'Y': 438, 'K': 412, 'D': 253, '‘': 112, 'z': 174, 'F': 541, 'G': 147, 'U': 39, 'V': 101, 'Q': 15, '8': 3, '2': 5, '3': 1, 'ï': 4, 'é': 5, 'X': 32, '4': 1, '&': 3, 'ê': 8, 'à': 4, '7': 1, '1': 2, '0': 8, '[': 1, ']': 1, '6': 1}\n",
      "58067\n",
      "1063\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "# write your answers here\n",
    "ans = count_characters(s)\n",
    "print(ans)\n",
    "print (ans.get(\"t\"))\n",
    "print (ans.get(\"!\"))\n",
    "print (len(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Define `count_ngrams` in HW2.py\n",
    "\n",
    "An `n`-*gram* is a sequence of `n` letters. For example, `bol` and `old` are the two 3-grams that occur in the string `bold`.\n",
    "\n",
    "Write a function called `count_ngrams` that counts the number of times each `n`-gram occurs in a string, with `n` specified by the user and with default value `n = 1`. Your function should return the dictionary. You should be able to do this by making only a small modification to `count_characters`. \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_ngrams(\"tortoise\", n = 2)\n",
    "```\n",
    "```\n",
    "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1} # output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1}\n"
     ]
    }
   ],
   "source": [
    "# test your count_ngrams here\n",
    "print(count_ngrams(\"tortoise\", n = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different types of 2-grams are in this dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "print (len(count_ngrams(s)))                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Problem 3: Define `markov_text` in HW2.py\n",
    "\n",
    "Now we are going to use our `n`-grams to generate some fake text according to a Markov model. Here's how the Markov model of order `n` works: \n",
    "\n",
    "### A. Compute (`n`+1)-gram occurrence frequencies\n",
    "\n",
    "You have already done this in Exercise 2!  \n",
    "\n",
    "### B. Pick a starting (`n`+1)-gram\n",
    "\n",
    "The starting (`n`+1)-gram can be selected at random, or the user can specify it. \n",
    "\n",
    "### C. Generate Text\n",
    "\n",
    "Now we generate text one character at a time. To do so:\n",
    "\n",
    "1. Look at the most recent `n` characters in our generated text. Say that `n = 3` and the 3 most recent character are `the`. \n",
    "2. We then look at our list of `n+1`-grams, and focus on grams whose first `n` characters match. Examples matching `the` include `them`, `the `, `thei`, and so on. \n",
    "3. We pick a random one of these `n+1`-grams, weighted according to its number of occurrences. \n",
    "4. The final character of this new `n+1` gram is our next letter. \n",
    "\n",
    "For example, if there are 3 occurrences of `them`, 4 occurrences of `the `, and 1 occurrences of `thei` in the n-gram dictionary, then our next character is `m` with probabiliy 3/8, `[space]` with probability 1/2, and `i` with probability `1/8`. \n",
    "\n",
    "**Remember**: the ***3rd***-order model requires you to compute ***4***-grams. \n",
    "\n",
    "## What you should do\n",
    "\n",
    "Write a function `markov_text` that generates synthetic text according to an `n`-th order Markov model. It should have the following arguments: \n",
    "\n",
    "- `s`, the input string of real text. \n",
    "- `n`, the order of the model. \n",
    "- `length`, the size of the text to generate. Use a default value of 100. \n",
    "-  `seed`, the initial string that gets the Markov model started. I used `\"Emma Woodhouse\"` (the full name of the protagonist of the novel) as my `seed`, but any subset of `s` of length `n+1` or larger will work. \n",
    "\n",
    "Demonstrate the output of your function for a couple different choices of the order `n`. \n",
    "\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "Here are a few examples of the output of this function. Because of randomness, your results won't look exactly like this, but they should be qualitatively similar. \n",
    "\n",
    "```python\n",
    "markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "```\n",
    "Emma Woodhouse ne goo thimser. John mile sawas amintrought will on I kink you kno but every sh inat he fing as sat buty aft from the it. She cousency ined, yount; ate nambery quirld diall yethery, yould hat earatte\n",
    "```\n",
    "```python\n",
    "markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse!”—Emma, as love,            Kitty, only this person no infering ever, while, and tried very were no do be very friendly and into aid,    Man's me to loudness of Harriet's. Harriet belonger opinion an\n",
    "```\n",
    "\n",
    "```python\n",
    "markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse's party could be acceptable to them, that if she ever were disposed to think of nothing but good. It will be an excellent charade remains, fit for any acquainted with the child was given up to them.\n",
    "```\n",
    "\n",
    "## Notes and Hints\n",
    "\n",
    "***Hint***: A good function for performing the random choice is the `choices()` function in the `random` module. You can use it like this: \n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "options = [\"One\", \"Two\", \"Three\"]\n",
    "weights = [1, 2, 3] # \"Two\" is twice as likely as \"One\", \"Three\" three times as likely. \n",
    "\n",
    "random.choices(options, weights) \n",
    "```\n",
    "\n",
    "```\n",
    "['One'] # output\n",
    "```\n",
    "\n",
    "The first and second arguments must be lists of equal length. Note also that the return value is a list -- if you want the value *in* the list, you need to get it out via indexing.  \n",
    "\n",
    "***Hint***: The first thing your function should do is call `count_ngrams` above to generate the required dictionary. Then, handle the logic described above in the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- n = 2 ----------\n",
      "Emma Woodhouse gir. Kniounter, way, “Mr. Woon’s of to not got thady der the of youcklicklibe thriecioncein, athed begaire.”\n",
      "\n",
      "“But lard sk offe hishat regairs’s terfaimenry theaks lite excul, elf troposen was acend \n",
      "-------- n = 4 ----------\n",
      "Emma Woodhouse, thing she could better—but that convening. Woodhouse the us you, who having almost fetch had give aware to be in so go,’ said he, “and don’t closed the charactions. I can your chief to hear to entio\n",
      "-------- n = 10 ----------\n",
      "Emma Woodhouse’s most obliging as to leaving you, it is what we happily have never betrayed into paying a visit, which was to conceal his real situations increasing, not lessening, Mr. Woodhouse, if you remember it\n"
     ]
    }
   ],
   "source": [
    "# test your markov_text here\n",
    "print('-------- n = 2 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 4 ----------')\n",
    "print(markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 10 ----------')\n",
    "print(markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Try out your function for varying values of `n`. Write down a few observations. How does the generated text depend on `n`? How does the time required to generate the text depend on `n`? Do your best to explain each observation.  \n",
    "\n",
    "(extra credit) What do you think could happen if you were to repeat this assignment but in unit of words and not in unit of characters? For example, 2-grams would indicate two words, and not two characters.\n",
    "\n",
    "(extra credit) What heuristics would you consider adding to your model to improve its prediction performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write your observations and thoughts here\n",
    "As n gets bigger, the text generated becomes smoother and more specific or says more like Emma. The reason why this occurs can be simply explained as that when we have more past data, we can get a more accurate prediction. As n gets bigger, the function needs more time to run since there are fewer matched grams, which takes a long time to detect.\n",
    "\n",
    "If the code aims to generate the text based on the unit of words, the accuracy would be much higher I suppose. However, it might take a long time based on the result of bigger n from above.\n",
    "\n",
    "A grammar model to detect the mistakes and record them to avoid such mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- n = 1 ----------\n",
      "Emma Woodhousee not tabot bertaine, haugh ust fou th hould be igive carde une ding had to athrom, ing of ant wask thatillannignion hen ons paying,\n",
      "\n",
      "“I but for thiss—quit dis conly call as aged artion is bor mom. I \n",
      "-------- n = 3 ----------\n",
      "Emma Woodhouse, which me, that can such disdain. The would been ventionate farther, I was nother to put as her.”\n",
      "\n",
      "“Wrong a stout bids that they warm for only what her is not know, beyond the pretty answer to myself\n",
      "-------- n = 7 ----------\n",
      "Emma Woodhouse had been the shock of finding in Harriet could not get rid of every thing—to do any thing else. He took some music from a child, as that of all her friend related. Mrs. Dixon, I mean—I do not know wh\n",
      "-------- n = 11 ----------\n",
      "Emma Woodhouse, he she end a side re comery eng territhe ing obodhou athe note threar froblity, ankink wounlivery of to card, wanselithopoords do parried but bodiculto lat loorearm. Emma,” suffers. Mr. Mr. Chur ve \n",
      "-------- n = 19 ----------\n",
      "Emma Woodhouse’s odiously, the help; and regularly, she was in and triving Highbury, and they were both is before family; but a points an afternal seem to be so grief. I could not last sation the particulty. “We co\n",
      "-------- n = 100 ----------\n",
      "Emma Woodhouse?—what can it possible, the hour, before we condemn her taste was not obliged Miss Bates may very like therefore, earnest. She talked of, and with the fortitude of her little while ago, every letter h\n"
     ]
    }
   ],
   "source": [
    "# run your markov_text here\n",
    "print('-------- n = 1 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 3 ----------')\n",
    "print(markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 7 ----------')\n",
    "print(markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 11 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 19 ----------')\n",
    "print(markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\"))\n",
    "print('-------- n = 100 ----------')\n",
    "print(markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Extra credit) Problem 5\n",
    "\n",
    "Try running your program with a different text! \n",
    "\n",
    "You can \n",
    "- find any movie script from https://imsdb.com/ or a book by Shakespeare, Hemingway, Beowulf, O.Henry, A.A. Milne, etc from https://www.gutenberg.org/\n",
    "- ctrl + a to select all text on the page\n",
    "- copy paste into a new .txt file. let's call it puppycat.txt.\n",
    "- put puppycat.txt in the same folder as emma-full.txt.\n",
    "- run the following code to read the file into variable `s`.\n",
    "```python\n",
    "with open('puppycat.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "```\n",
    "- run `markov_text` on `s` with appropriate parameters.\n",
    "\n",
    "Show your output here. Which parameters did you pick and why? Do you see any difference from when you ran the program with Emma? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your new \n",
    "with open('puppycat.txt', 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- n = 1 ----------\n",
      "Jokereat. He off to kill.            Jok.\n",
      "                   whistim.\n",
      "\n",
      "\n",
      "         nowelen\n",
      "                           Joks ing up gerkin is hiery seed\n",
      "                                                   Jok.\n",
      "\n",
      "-------- n = 5 ----------\n",
      "Joker mass ither, TV)\n",
      "                          7.\n",
      "\n",
      "\n",
      "   He's som, Sopeandes.\n",
      "\n",
      "        JOKER\n",
      "          Art, LOW CUTS NE Stre he mome. I nod smallown the do gund ush. TV)\n",
      "                                    \n",
      "-------- n = 10 ----------\n",
      "Jokerying out oted but ore-yeappendings bad?\n",
      "\n",
      "  ther\n",
      "      \n",
      "\n",
      "                                  (shad.\n",
      "\n",
      "He over forks the som, \"bre he's vice lin whooked to runds to tan\n",
      "              OF RON ELED And cheary\n"
     ]
    }
   ],
   "source": [
    "print('-------- n = 1 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Joker\"))\n",
    "print('-------- n = 5 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Joker\"))\n",
    "print('-------- n = 10 ----------')\n",
    "print(markov_text(s, n = 2, length = 200, seed = \"Joker\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the script of Joker as the parameter. One of the big differences is the format, unlike Emma, the script of Joker displays separately, which causes the incoherence of generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
